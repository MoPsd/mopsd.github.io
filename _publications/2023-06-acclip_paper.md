---
title: "Stochastic First-Order Learning for Large-Scale Flexibly-Tied Gaussian Mixture Model"
collection: publications
date: 2024-02-01
paperurl: 'https://doi.org/10.1016/j.patrec.2023.12.021 '
citation: 'Pasande, M., Hosseini, R., & Araabi, B. N. (2024). Stochastic first-order learning for large-scale flexibly tied Gaussian mixture models. Pattern Recognition Letters, 178, 138-144.'
---
<!-- This paper is corresponding to the subject of my Ms.c Thesis and it's Under Preparation.
![Editing a markdown file for a talk](/images/glow_mine.gif) -->

**Abstract**: Gaussian Mixture Models (GMMs) are one of the most potent parametric density models used extensively in many applications. Flexibly-tied factorization of the covariance matrices in GMMs is a powerful approach for coping with the challenges of common GMMs when faced with high-dimensional data and complex densities which often demand a large number of Gaussian components. However, the expectation–maximization algorithm for fitting flexibly-tied GMMs still encounters difficulties with streaming and very large dimensional data. To overcome these challenges, this paper suggests the use of first-order stochastic optimization algorithms. Specifically, we propose a new stochastic optimization algorithm on the manifold of orthogonal matrices. Through numerous empirical results on both synthetic and real datasets, we observe that stochastic optimization methods can outperform the expectation–maximization algorithm in terms of attaining better likelihood, needing fewer epochs for convergence, and consuming less time per each epoch.



![](/images/largeGMM.PNG)



This paper corresponds to the first phase of my M.Sc Thesis.

